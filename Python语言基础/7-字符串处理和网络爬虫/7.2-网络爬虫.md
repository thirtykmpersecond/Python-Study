网络爬虫执行流程为：获取网页源码、从源码中提取相关的信息、进行数据存储。

# 获取网页源码
通过Python内置的`urllib.request`模块可以轻松获取网页的字节码，通过对字节码的解码可以获取到网页的源码字符串。
```python
from urllib import request
fp = request.urlopen('http://www.nuc.edu.cn')
fp

content = fp.read()
content

fp.close()
type(content)

html = content.decode()
html

### 并不是所有网页都采用utf-8编码，需要用chardet库判断编码方式
import chardet
det = chardet.detect(content)
det

if det['confidence'] > 0.8 :
    html = content.decode(det['encoding'])
    print(det['encoding'])
else :
    html = content.decode('gbk')
    print(det['encoding'])
``` 

# 从源码中获取信息
面我们介绍了正则表达式，如果一个正则匹配稍有差池，可能程序就会困在永久的循环之中，而且有些读者对写正则表达式还是很犯怵的，没关系，己经有好心人替我们做了不少准备工作。
`Beautiful Soup`可以方便提取`HTML`和`XML`标签中的内容。

`Beautiful Soup`是一个可以从`HTML`和`XML`文件中提取数据的Python 库，其最主要的功能就是从网页抓取数据。Beautiful Soup 提供了一些简单的、Python式的函数，用来处理导航、搜索、修改分析树等功能。
它是一个工具箱，通过解析文档为用户提供需要抓取的数据。因为比较简单，所以不需要多少代码就可以写出一个完整的应用程序。Beautiful Soup 自动将输入文档转换为unicode编码，输出文档转换为utf-8编码，不需要考虑编码方式。
```python
conda install beautifulsoup4

from bs4 import BeautifulSoup
html = """
<html><head><title>The Dormouse's story</title></head>
<body><p class='title'><b>The Dormouse's story</b></p>
<p class="story">Once upon a time there were three little sisters; and their names were
<a href="http://example.com/elsie" class="sister" id="link1">Elsie</a>
<a href="http://example.com/lacie" class="sister" id="link2">Lacie</a>
<a href="http://example.com/tillie" class="sister" id="link3">Tillie</a>
and they lived at the bottom of a well.</p>
<p class="story">...</p>
"""
soup = BeautifulSoup(html)
print(soup.prettify())

### 找出所有a标签
for a in soup.findAll(name='a') :
    print('attrs: ', a.attrs)
    print('string: ', a.string)
    print('----------------')

### 找出所有class = 'sister', id = 'link1'的标签
for tag in soup.findAll(attrs = {'class':'sister', 'id':'link1'}) :
    print('tag: ', tag.name)
    print('attrs: ', tag.attrs)
    print('string: ', tag.string)

### 找出所有包含内容为Elsie的标签
for tag in soup.findAll(name='a', text="Elsie"):
    print('tag: ', tag.name)
    print('attrs: ', tag.attrs)
    print('string: ', tag.string)

### 用正则找出所有id='link数字'的标签
import re
for tag in  soup.findAll(attrs={'id':re.compile('link\d')}) :
    print(tag)

### 用正则找出所有包含内容结尾为ie的a标签
for a in soup.findAll('a', text=re.compile('.*?ie')) :
    print(tag)

### 解析出标签名为a，属性部位空气且id属性为link1的标签
def parser(tag) :
    if tag.name == 'a' and tag.attrs and tag.attrs['id'] == 'link1' :
        return True
for tag in soup.findAll(parser) :
    print(tag)
```

# 数据存储

## 保存到csv
Python自带csv模块可以处理csv。
```python
csv = '''id,name,score
1,xiaohua,23
2,xiaoming,67
3,xiaogang,89'''

with open('/Users/pain/Desktop/aa.csv', 'w') as f:
    f.write(csv)
```

## 保存到数据库
```python
import sqlite3 as base

### 数据库文件存在时，直接连接；不存在时，则应创建相应数据库文件
db = base.connect('/Users/pain/Desktop/test.db')

### 获取游标
sur = db.cursor()

### 建表
sur.execute("""create table info (
            id text,
            name text,
            score text)""")

db.commit()
### 添加数据。
sur.execute("insert into info values ('1', 'xiaohua', '23')")
sur.execute("insert into info values ('2', 'xiaoming', '67')")
sur.execute("insert into info values ('3', 'xiaogang', '89')")

db.commit()
sur.close()
db.close()
```

## 网络爬虫
### 纯手工打造网络爬虫
对`https://movie.douban.com/top250`进行爬取。
